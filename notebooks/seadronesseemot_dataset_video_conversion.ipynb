{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d8d974",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-26T15:56:10.169307Z",
     "iopub.status.busy": "2025-12-26T15:56:10.168794Z",
     "iopub.status.idle": "2025-12-26T15:56:10.758337Z",
     "shell.execute_reply": "2025-12-26T15:56:10.757263Z"
    },
    "papermill": {
     "duration": 0.596981,
     "end_time": "2025-12-26T15:56:10.760841",
     "exception": false,
     "start_time": "2025-12-26T15:56:10.163860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e266fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T15:56:10.768003Z",
     "iopub.status.busy": "2025-12-26T15:56:10.767648Z",
     "iopub.status.idle": "2025-12-26T15:56:10.792323Z",
     "shell.execute_reply": "2025-12-26T15:56:10.791251Z"
    },
    "papermill": {
     "duration": 0.031678,
     "end_time": "2025-12-26T15:56:10.794806",
     "exception": false,
     "start_time": "2025-12-26T15:56:10.763128",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_unified_mot_dataset():\n",
    "    # Paths to annotation files\n",
    "    json_paths = {\n",
    "        'train': \"/kaggle/input/seadronesseemot-annotation/annotations/annotations/instances_train_objects_in_water_life_jacket_rm_fixed.json\",\n",
    "        'val': \"/kaggle/input/seadronesseemot-annotation/annotations/annotations/instances_val_objects_in_fixed.json\", \n",
    "        'test': \"/kaggle/input/seadronesseemot-annotation/annotations/annotations/instances_test_objects_in_water_fixed.json\"\n",
    "    }\n",
    "    \n",
    "    # Load all datasets\n",
    "    datasets = {}\n",
    "    for split, path in json_paths.items():\n",
    "        print(f\"Loading {split}: {path}\")\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                with open(path, 'r') as f:\n",
    "                    datasets[split] = json.load(f)\n",
    "                print(f\"  âœ“ Loaded {split}: {len(datasets[split].get('images', []))} images, {len(datasets[split].get('annotations', []))} annotations\")\n",
    "            except Exception as e:\n",
    "                print(f\"  âœ— Error loading {split}: {e}\")\n",
    "                datasets[split] = {'images': [], 'annotations': []}\n",
    "        else:\n",
    "            print(f\"  âš  Warning: {path} not found\")\n",
    "            datasets[split] = {'images': [], 'annotations': []}\n",
    "    \n",
    "    # Create mappings for quick lookup\n",
    "    img_by_id = {}\n",
    "    ann_by_img = defaultdict(list)\n",
    "    video_frames = defaultdict(list)\n",
    "    \n",
    "    # Process all splits\n",
    "    for split, data in datasets.items():\n",
    "        print(f\"\\nProcessing {split} split...\")\n",
    "        \n",
    "        # Safely get images\n",
    "        images = data.get('images', [])\n",
    "        if images is None:\n",
    "            print(f\"  Warning: 'images' is None in {split}, using empty list\")\n",
    "            images = []\n",
    "        \n",
    "        # Index images by ID\n",
    "        for img in images:\n",
    "            if isinstance(img, dict) and 'id' in img:\n",
    "                img_id = img['id']\n",
    "                img_by_id[img_id] = img\n",
    "                video_frames[img['video_id']].append(img)\n",
    "            else:\n",
    "                print(f\"  Warning: Invalid image entry in {split}: {img}\")\n",
    "        \n",
    "        # Safely get annotations\n",
    "        annotations = data.get('annotations', [])\n",
    "        if annotations is None:\n",
    "            print(f\"  Warning: 'annotations' is None in {split}, using empty list\")\n",
    "            annotations = []\n",
    "        \n",
    "        # Index annotations by image_id\n",
    "        for ann in annotations:\n",
    "            if isinstance(ann, dict) and 'image_id' in ann:\n",
    "                ann_by_img[ann['image_id']].append(ann)\n",
    "            else:\n",
    "                print(f\"  Warning: Invalid annotation in {split}: {ann}\")\n",
    "    \n",
    "    print(f\"\\nFound {len(video_frames)} unique video IDs\")\n",
    "    \n",
    "    # Get all unique video IDs\n",
    "    all_video_ids = sorted(video_frames.keys())\n",
    "    \n",
    "    # Create unified structure\n",
    "    unified_data = {\n",
    "        'info': datasets.get('train', {}).get('info', {}),\n",
    "        'licenses': datasets.get('train', {}).get('licenses', []),\n",
    "        'categories': datasets.get('train', {}).get('categories', []),\n",
    "        'videos': [],\n",
    "        'images': [],\n",
    "        'annotations': []\n",
    "    }\n",
    "    \n",
    "    global_image_id = 0\n",
    "    global_ann_id = 0\n",
    "    \n",
    "    for video_id in all_video_ids:\n",
    "        print(f\"Processing video {video_id}...\")\n",
    "        \n",
    "        # Create video entry\n",
    "        video_entry = {\n",
    "            'id': int(video_id),\n",
    "            'name': f\"video_{video_id}\",\n",
    "            'width': 3840,  # From sample data\n",
    "            'height': 2160  # From sample data\n",
    "        }\n",
    "        unified_data['videos'].append(video_entry)\n",
    "        \n",
    "        # Get all frames for this video, sorted by frame_index\n",
    "        frames = video_frames[video_id]\n",
    "        frames.sort(key=lambda x: x.get('frame_index', 0))\n",
    "        \n",
    "        video_frame_count = 0\n",
    "        video_ann_count = 0\n",
    "        \n",
    "        for frame in frames:\n",
    "            # Create new global image ID to avoid conflicts\n",
    "            new_img_id = global_image_id\n",
    "            global_image_id += 1\n",
    "            \n",
    "            # Copy frame info with updated ID\n",
    "            new_frame = frame.copy()\n",
    "            new_frame['id'] = new_img_id\n",
    "            new_frame['video_id'] = int(video_id)\n",
    "            unified_data['images'].append(new_frame)\n",
    "            video_frame_count += 1\n",
    "            \n",
    "            # Get annotations for this frame (using original image ID)\n",
    "            orig_img_id = frame['id']\n",
    "            orig_ann_list = ann_by_img.get(orig_img_id, [])\n",
    "            \n",
    "            for orig_ann in orig_ann_list:\n",
    "                new_ann_id_local = global_ann_id\n",
    "                global_ann_id += 1\n",
    "                \n",
    "                # Create new annotation\n",
    "                new_ann = {\n",
    "                    'id': new_ann_id_local,\n",
    "                    'image_id': new_img_id,\n",
    "                    'video_id': int(video_id),\n",
    "                    'category_id': orig_ann.get('category_id'),\n",
    "                    'track_id': orig_ann.get('track_id'),\n",
    "                    'bbox': orig_ann.get('bbox'),\n",
    "                    'area': orig_ann.get('area', 0),\n",
    "                    'iscrowd': orig_ann.get('iscrowd', 0)\n",
    "                }\n",
    "                \n",
    "                unified_data['annotations'].append(new_ann)\n",
    "                video_ann_count += 1\n",
    "        \n",
    "        print(f\"  Video {video_id}: {video_frame_count} frames, {video_ann_count} annotations\")\n",
    "    \n",
    "    # Save unified dataset\n",
    "    output_path = \"/kaggle/working/unified_mot_dataset.json\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(unified_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ“ Unified MOT dataset saved to: {output_path}\")\n",
    "    print(f\"Total videos: {len(unified_data['videos'])}\")\n",
    "    print(f\"Total images: {len(unified_data['images'])}\")\n",
    "    print(f\"Total annotations: {len(unified_data['annotations'])}\")\n",
    "    \n",
    "    return unified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0186f515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T15:56:10.800834Z",
     "iopub.status.busy": "2025-12-26T15:56:10.800489Z",
     "iopub.status.idle": "2025-12-26T15:56:18.329937Z",
     "shell.execute_reply": "2025-12-26T15:56:18.328927Z"
    },
    "papermill": {
     "duration": 7.535341,
     "end_time": "2025-12-26T15:56:18.332299",
     "exception": false,
     "start_time": "2025-12-26T15:56:10.796958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train: /kaggle/input/seadronesseemot-annotation/annotations/annotations/instances_train_objects_in_water_life_jacket_rm_fixed.json\n",
      "  âœ“ Loaded train: 27259 images, 159454 annotations\n",
      "Loading val: /kaggle/input/seadronesseemot-annotation/annotations/annotations/instances_val_objects_in_fixed.json\n",
      "  âœ“ Loaded val: 8584 images, 47678 annotations\n",
      "Loading test: /kaggle/input/seadronesseemot-annotation/annotations/annotations/instances_test_objects_in_water_fixed.json\n",
      "  âœ— Error loading test: object of type 'NoneType' has no len()\n",
      "\n",
      "Processing train split...\n",
      "\n",
      "Processing val split...\n",
      "\n",
      "Processing test split...\n",
      "\n",
      "Found 21 unique video IDs\n",
      "Processing video 0...\n",
      "  Video 0: 410 frames, 1856 annotations\n",
      "Processing video 1...\n",
      "  Video 1: 715 frames, 10072 annotations\n",
      "Processing video 2...\n",
      "  Video 2: 715 frames, 3550 annotations\n",
      "Processing video 4...\n",
      "  Video 4: 3490 frames, 22934 annotations\n",
      "Processing video 5...\n",
      "  Video 5: 715 frames, 6435 annotations\n",
      "Processing video 6...\n",
      "  Video 6: 2098 frames, 5275 annotations\n",
      "Processing video 7...\n",
      "  Video 7: 1001 frames, 7042 annotations\n",
      "Processing video 8...\n",
      "  Video 8: 1001 frames, 6745 annotations\n",
      "Processing video 9...\n",
      "  Video 9: 3709 frames, 26549 annotations\n",
      "Processing video 10...\n",
      "  Video 10: 715 frames, 3993 annotations\n",
      "Processing video 11...\n",
      "  Video 11: 608 frames, 1180 annotations\n",
      "Processing video 12...\n",
      "  Video 12: 715 frames, 5659 annotations\n",
      "Processing video 13...\n",
      "  Video 13: 291 frames, 1455 annotations\n",
      "Processing video 14...\n",
      "  Video 14: 1726 frames, 9042 annotations\n",
      "Processing video 15...\n",
      "  Video 15: 252 frames, 357 annotations\n",
      "Processing video 16...\n",
      "  Video 16: 1429 frames, 10219 annotations\n",
      "Processing video 17...\n",
      "  Video 17: 1429 frames, 6644 annotations\n",
      "Processing video 18...\n",
      "  Video 18: 6028 frames, 17249 annotations\n",
      "Processing video 19...\n",
      "  Video 19: 913 frames, 1338 annotations\n",
      "Processing video 20...\n",
      "  Video 20: 13 frames, 0 annotations\n",
      "Processing video 21...\n",
      "  Video 21: 7870 frames, 59538 annotations\n",
      "\n",
      "âœ“ Unified MOT dataset saved to: /kaggle/working/unified_mot_dataset.json\n",
      "Total videos: 21\n",
      "Total images: 35843\n",
      "Total annotations: 207132\n"
     ]
    }
   ],
   "source": [
    "# Run the function\n",
    "if __name__ == \"__main__\":\n",
    "    unified_dataset = create_unified_mot_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea4ad7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T15:56:18.339764Z",
     "iopub.status.busy": "2025-12-26T15:56:18.339333Z",
     "iopub.status.idle": "2025-12-26T15:56:18.355110Z",
     "shell.execute_reply": "2025-12-26T15:56:18.354256Z"
    },
    "papermill": {
     "duration": 0.022269,
     "end_time": "2025-12-26T15:56:18.357367",
     "exception": false,
     "start_time": "2025-12-26T15:56:18.335098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_videos_for_specific_ids(target_video_ids=None, fps=25):\n",
    "    \"\"\"\n",
    "    Create videos for specific video IDs from unified_mot_dataset.json\n",
    "    \n",
    "    Args:\n",
    "        target_video_ids: list of video IDs to process (e.g., [0, 5, 7]), None for all\n",
    "        fps: frames per second for output videos\n",
    "    \"\"\"\n",
    "    UNIFIED_JSON = \"/kaggle/working/unified_mot_dataset.json\"\n",
    "    \n",
    "    # Separate image folders for each split\n",
    "    IMAGE_FOLDERS = {\n",
    "        'train': \"/kaggle/input/seadronesseemot-train/train\",\n",
    "        'val': \"/kaggle/input/seadronesseemot-validation/val\", \n",
    "        'test': \"/kaggle/input/seadronesseemot-test/test\"\n",
    "    }\n",
    "    \n",
    "    OUTPUT_DIR = \"/kaggle/working/videos\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Load unified dataset\n",
    "    with open(UNIFIED_JSON, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    images = data['images']\n",
    "    videos = data['videos']\n",
    "    \n",
    "    # Group images by original video_id\n",
    "    images_by_video = defaultdict(list)\n",
    "    for img in images:\n",
    "        images_by_video[img['video_id']].append(img)\n",
    "    \n",
    "    # Filter video IDs\n",
    "    if target_video_ids is None:\n",
    "        target_video_ids = [v['id'] for v in videos]\n",
    "    else:\n",
    "        target_video_ids = [int(vid) for vid in target_video_ids]\n",
    "    \n",
    "    print(f\"Creating videos for video IDs: {target_video_ids}\")\n",
    "    \n",
    "    successful_videos = 0\n",
    "    \n",
    "    for video_id in target_video_ids:\n",
    "        print(f\"\\n--- Processing video {video_id} ---\")\n",
    "        \n",
    "        # Get frames for this video\n",
    "        frames = images_by_video.get(video_id, [])\n",
    "        if not frames:\n",
    "            print(f\"  No frames found for video {video_id}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Sort by frame_index\n",
    "        frames.sort(key=lambda x: x.get('frame_index', 0))\n",
    "        print(f\"  Found {len(frames)} frames\")\n",
    "        \n",
    "        # Build frame paths (check all possible split folders)\n",
    "        frame_paths = []\n",
    "        missing_frames = 0\n",
    "        \n",
    "        for frame in frames:\n",
    "            file_name = frame['file_name']\n",
    "            \n",
    "            # Try to find image in train/val/test folders\n",
    "            frame_path = None\n",
    "            for split_name, folder_path in IMAGE_FOLDERS.items():\n",
    "                candidate_path = os.path.join(folder_path, file_name)\n",
    "                if os.path.exists(candidate_path):\n",
    "                    frame_path = candidate_path\n",
    "                    break\n",
    "            \n",
    "            if frame_path and os.path.exists(frame_path):\n",
    "                frame_paths.append((frame_path, frame['frame_index']))\n",
    "            else:\n",
    "                print(f\"  WARNING: Missing frame '{file_name}' (frame_index: {frame.get('frame_index', 'N/A')})\")\n",
    "                missing_frames += 1\n",
    "        \n",
    "        if missing_frames > 0:\n",
    "            print(f\"  {missing_frames}/{len(frames)} frames missing\")\n",
    "        \n",
    "        if len(frame_paths) < 2:  # Need at least 2 frames for video\n",
    "            print(f\"  Too few valid frames ({len(frame_paths)}), skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Sort valid frames by frame_index\n",
    "        frame_paths.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Read first frame to get dimensions\n",
    "        first_frame_path, _ = frame_paths[0]\n",
    "        first_frame = cv2.imread(first_frame_path)\n",
    "        if first_frame is None:\n",
    "            print(f\"  Could not read first frame {first_frame_path}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        height, width = first_frame.shape[:2]\n",
    "        print(f\"  Video dimensions: {width}x{height}\")\n",
    "        \n",
    "        # Create video writer\n",
    "        output_path = os.path.join(OUTPUT_DIR, f\"video_{video_id:03d}.mp4\")\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "        \n",
    "        # Write frames\n",
    "        valid_frames_written = 0\n",
    "        for frame_path, frame_idx in frame_paths:\n",
    "            frame = cv2.imread(frame_path)\n",
    "            if frame is None:\n",
    "                print(f\"  Failed to read frame at index {frame_idx}\")\n",
    "                continue\n",
    "            \n",
    "            # Resize if dimensions don't match\n",
    "            if frame.shape[0] != height or frame.shape[1] != width:\n",
    "                frame = cv2.resize(frame, (width, height))\n",
    "            \n",
    "            writer.write(frame)\n",
    "            valid_frames_written += 1\n",
    "        \n",
    "        writer.release()\n",
    "        \n",
    "        print(f\"  âœ“ Saved {valid_frames_written} frames to {output_path}\")\n",
    "        successful_videos += 1\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Successfully created {successful_videos}/{len(target_video_ids)} videos\")\n",
    "    print(f\"ðŸ“ Videos saved in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10de4210",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T15:56:18.364125Z",
     "iopub.status.busy": "2025-12-26T15:56:18.363794Z",
     "iopub.status.idle": "2025-12-26T16:51:43.434741Z",
     "shell.execute_reply": "2025-12-26T16:51:43.430473Z"
    },
    "papermill": {
     "duration": 3325.085135,
     "end_time": "2025-12-26T16:51:43.445033",
     "exception": false,
     "start_time": "2025-12-26T15:56:18.359898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating videos for video IDs: [0, 5, 7, 8, 9, 12, 13, 14, 15, 18, 19, 21]\n",
      "\n",
      "--- Processing video 0 ---\n",
      "  Found 410 frames\n",
      "  Video dimensions: 3840x2160\n",
      "  âœ“ Saved 410 frames to /kaggle/working/videos/video_000.mp4\n",
      "\n",
      "--- Processing video 5 ---\n",
      "  Found 715 frames\n",
      "  Video dimensions: 3840x2160\n",
      "  âœ“ Saved 715 frames to /kaggle/working/videos/video_005.mp4\n",
      "\n",
      "--- Processing video 7 ---\n",
      "  Found 1001 frames\n",
      "  Video dimensions: 3840x2160\n",
      "  âœ“ Saved 1001 frames to /kaggle/working/videos/video_007.mp4\n",
      "\n",
      "--- Processing video 8 ---\n",
      "  Found 1001 frames\n",
      "  Video dimensions: 3840x2160\n",
      "  âœ“ Saved 1001 frames to /kaggle/working/videos/video_008.mp4\n",
      "\n",
      "--- Processing video 9 ---\n",
      "  Found 3709 frames\n",
      "  Video dimensions: 3840x2160\n",
      "  âœ“ Saved 3709 frames to /kaggle/working/videos/video_009.mp4\n",
      "\n",
      "--- Processing video 12 ---\n",
      "  Found 715 frames\n",
      "  Video dimensions: 3840x2160\n",
      "  âœ“ Saved 715 frames to /kaggle/working/videos/video_012.mp4\n",
      "\n",
      "--- Processing video 13 ---\n",
      "  Found 291 frames\n",
      "  Video dimensions: 3840x2160\n",
      "  âœ“ Saved 291 frames to /kaggle/working/videos/video_013.mp4\n",
      "\n",
      "--- Processing video 14 ---\n",
      "  Found 1726 frames\n",
      "  Video dimensions: 3840x2160\n",
      "  âœ“ Saved 1726 frames to /kaggle/working/videos/video_014.mp4\n",
      "\n",
      "--- Processing video 15 ---\n",
      "  Found 252 frames\n",
      "  Video dimensions: 3840x2160\n",
      "  âœ“ Saved 252 frames to /kaggle/working/videos/video_015.mp4\n",
      "\n",
      "--- Processing video 18 ---\n",
      "  Found 6028 frames\n",
      "  Video dimensions: 3840x2160\n",
      "  âœ“ Saved 6028 frames to /kaggle/working/videos/video_018.mp4\n",
      "\n",
      "--- Processing video 19 ---\n",
      "  Found 913 frames\n",
      "  Video dimensions: 3840x2160\n",
      "  âœ“ Saved 913 frames to /kaggle/working/videos/video_019.mp4\n",
      "\n",
      "--- Processing video 21 ---\n",
      "  Found 7870 frames\n",
      "  Video dimensions: 3840x2160\n",
      "  âœ“ Saved 7870 frames to /kaggle/working/videos/video_021.mp4\n",
      "\n",
      "ðŸŽ‰ Successfully created 12/12 videos\n",
      "ðŸ“ Videos saved in: /kaggle/working/videos\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "\n",
    "# 1. Create videos for specific video IDs\n",
    "create_videos_for_specific_ids(target_video_ids=[0, 5, 7, 8, 9, 12, 13, 14, 15, 18, 19, 21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebceab8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-26T16:51:43.460654Z",
     "iopub.status.busy": "2025-12-26T16:51:43.459876Z",
     "iopub.status.idle": "2025-12-26T16:56:49.370411Z",
     "shell.execute_reply": "2025-12-26T16:56:49.368285Z"
    },
    "papermill": {
     "duration": 305.923537,
     "end_time": "2025-12-26T16:56:49.373297",
     "exception": false,
     "start_time": "2025-12-26T16:51:43.449760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/videos/ (stored 0%)\r\n",
      "  adding: kaggle/working/videos/video_009.mp4 (deflated 1%)\r\n",
      "  adding: kaggle/working/videos/video_005.mp4 (deflated 1%)\r\n",
      "  adding: kaggle/working/videos/video_018.mp4 (deflated 1%)\r\n",
      "  adding: kaggle/working/videos/video_015.mp4 (deflated 2%)\r\n",
      "  adding: kaggle/working/videos/video_007.mp4 (deflated 1%)\r\n",
      "  adding: kaggle/working/videos/video_014.mp4 (deflated 1%)\r\n",
      "  adding: kaggle/working/videos/video_000.mp4 (deflated 0%)\r\n",
      "  adding: kaggle/working/videos/video_013.mp4 (deflated 0%)\r\n",
      "  adding: kaggle/working/videos/video_021.mp4 (deflated 2%)\r\n",
      "  adding: kaggle/working/videos/video_012.mp4 (deflated 0%)\r\n",
      "  adding: kaggle/working/videos/video_019.mp4 (deflated 1%)\r\n",
      "  adding: kaggle/working/videos/video_008.mp4 (deflated 0%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip -r videos.zip /kaggle/working/videos"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8056616,
     "sourceId": 12744882,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8056853,
     "sourceId": 12745230,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8056871,
     "sourceId": 12745257,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9129410,
     "sourceId": 14301442,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3644.974952,
   "end_time": "2025-12-26T16:56:51.019178",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-26T15:56:06.044226",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
