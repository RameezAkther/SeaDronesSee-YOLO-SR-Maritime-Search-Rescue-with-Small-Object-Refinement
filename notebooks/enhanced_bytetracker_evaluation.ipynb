{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q ultralytics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-27T09:56:13.902807Z","iopub.execute_input":"2025-12-27T09:56:13.903094Z","iopub.status.idle":"2025-12-27T09:56:19.429435Z","shell.execute_reply.started":"2025-12-27T09:56:13.903061Z","shell.execute_reply":"2025-12-27T09:56:19.428638Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q supervision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T09:56:19.430720Z","iopub.execute_input":"2025-12-27T09:56:19.431032Z","iopub.status.idle":"2025-12-27T09:56:23.048489Z","shell.execute_reply.started":"2025-12-27T09:56:19.431002Z","shell.execute_reply":"2025-12-27T09:56:23.047802Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m212.4/212.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install \"numpy<2.0\" --force-reinstall","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T09:56:23.050266Z","iopub.execute_input":"2025-12-27T09:56:23.050536Z","iopub.status.idle":"2025-12-27T09:56:29.719461Z","shell.execute_reply.started":"2025-12-27T09:56:23.050509Z","shell.execute_reply":"2025-12-27T09:56:29.718709Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy<2.0\n  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!unzip /kaggle/input/seadronesseemot-dataset-video-conversion/videos.zip -d /kaggle/working/videos","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T09:58:03.053623Z","iopub.execute_input":"2025-12-27T09:58:03.054202Z","iopub.status.idle":"2025-12-27T09:59:09.400676Z","shell.execute_reply.started":"2025-12-27T09:58:03.054169Z","shell.execute_reply":"2025-12-27T09:59:09.399750Z"}},"outputs":[{"name":"stdout","text":"Archive:  /kaggle/input/seadronesseemot-dataset-video-conversion/videos.zip\n   creating: /kaggle/working/videos/kaggle/working/videos/\n  inflating: /kaggle/working/videos/kaggle/working/videos/video_009.mp4  \n  inflating: /kaggle/working/videos/kaggle/working/videos/video_005.mp4  \n  inflating: /kaggle/working/videos/kaggle/working/videos/video_018.mp4  \n  inflating: /kaggle/working/videos/kaggle/working/videos/video_015.mp4  \n  inflating: /kaggle/working/videos/kaggle/working/videos/video_007.mp4  \n  inflating: /kaggle/working/videos/kaggle/working/videos/video_014.mp4  \n  inflating: /kaggle/working/videos/kaggle/working/videos/video_000.mp4  \n  inflating: /kaggle/working/videos/kaggle/working/videos/video_013.mp4  \n  inflating: /kaggle/working/videos/kaggle/working/videos/video_021.mp4  \n  inflating: /kaggle/working/videos/kaggle/working/videos/video_012.mp4  \n  inflating: /kaggle/working/videos/kaggle/working/videos/video_019.mp4  \n  inflating: /kaggle/working/videos/kaggle/working/videos/video_008.mp4  \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom collections import deque, defaultdict\nimport supervision as sv\nfrom ultralytics import YOLO\nfrom pathlib import Path\nimport os\nimport json\nimport re\n\n# ‚úÖ 1. NOVEL TRACKER (Unchanged logic)\nclass NovelByteTrack(sv.ByteTrack):\n    def __init__(self, track_thresh=0.5, track_buffer=30, match_thresh=0.8, frame_rate=30):\n        super().__init__(\n            track_activation_threshold=track_thresh,\n            lost_track_buffer=track_buffer,\n            minimum_matching_threshold=match_thresh,\n            frame_rate=frame_rate\n        )\n        self.track_low_thresh = 0.1\n        self.kalman_filters = {}\n        self.track_histories = {}\n        self.speed_factor = 0.3\n        \n    def update_with_detections(self, detections):\n        detections = self._drone_preprocessing(detections)\n        if len(detections) == 0:\n            return detections\n        tracked_detections = super().update_with_detections(detections)\n        tracked_detections = self._novel_postprocessing(tracked_detections)\n        return tracked_detections\n    \n    def _drone_preprocessing(self, detections):\n        if len(detections) == 0: return detections\n        boxes = detections.xyxy\n        confs = detections.confidence\n        w = boxes[:, 2] - boxes[:, 0]\n        h = boxes[:, 3] - boxes[:, 1]\n        aspect_ratio = w / (h + 1e-6)\n        valid_mask = (aspect_ratio < 3.0) & (aspect_ratio > 0.3) & (confs > 0.1)\n        if np.sum(valid_mask) == 0: return sv.Detections.empty()\n        return sv.Detections(xyxy=boxes[valid_mask], confidence=confs[valid_mask], class_id=detections.class_id[valid_mask])\n    \n    def _novel_postprocessing(self, detections):\n        if len(detections) == 0 or detections.tracker_id is None: return detections\n        valid_tracks = detections.tracker_id != -1\n        if not np.any(valid_tracks): return detections\n        valid_ids = detections.tracker_id[valid_tracks]\n        valid_boxes = detections.xyxy[valid_tracks]\n        valid_confs = detections.confidence[valid_tracks]\n        enhanced = valid_confs.copy()\n        for i, tid in enumerate(valid_ids):\n            tid = int(tid)\n            cx, cy = (valid_boxes[i][0] + valid_boxes[i][2])/2, (valid_boxes[i][1] + valid_boxes[i][3])/2\n            kf = self._get_kalman_filter(tid, cx, cy)\n            kf.predict()\n            kf.correct(np.array([[cx], [cy]], dtype=np.float32))\n            self._update_track_history(tid, cx, cy, valid_confs[i])\n            enhanced[i] *= self._compute_speed_score(tid)\n        detections.confidence[valid_tracks] = enhanced\n        return detections\n    \n    def _get_kalman_filter(self, tid, x, y):\n        if tid not in self.kalman_filters:\n            kf = cv2.KalmanFilter(4, 2)\n            kf.transitionMatrix = np.eye(4, dtype=np.float32); kf.transitionMatrix[0,2]=1; kf.transitionMatrix[1,3]=1\n            kf.measurementMatrix = np.eye(2, 4, dtype=np.float32)\n            kf.processNoiseCov = np.eye(4, dtype=np.float32) * 0.03\n            kf.measurementNoiseCov = np.eye(2, dtype=np.float32) * 0.1\n            kf.statePre = np.array([[x], [y], [0], [0]], dtype=np.float32)\n            kf.errorCovPre = np.eye(4, dtype=np.float32)\n            self.kalman_filters[tid] = kf\n        return self.kalman_filters[tid]\n    \n    def _update_track_history(self, tid, x, y, conf):\n        if tid not in self.track_histories: self.track_histories[tid] = deque(maxlen=10)\n        self.track_histories[tid].append((x, y, conf))\n    \n    def _compute_speed_score(self, tid):\n        if tid not in self.track_histories or len(self.track_histories[tid]) < 2: return 1.0\n        pos = list(self.track_histories[tid])\n        d = np.linalg.norm(np.array(pos[-1][:2]) - np.array(pos[-2][:2]))\n        return np.clip(1.0 - (d / 15.0) * self.speed_factor, 0.3, 1.0)\n\n# ‚úÖ 2. FIXED REAL-TIME EVALUATOR\nclass RealTimeEvaluator:\n    def __init__(self, iou_threshold=0.5):\n        self.iou_threshold = iou_threshold\n        self.reset_stats()\n        # Structure: self.gt_data[video_id][frame_index] = list of objects\n        self.gt_data = defaultdict(lambda: defaultdict(list))\n        \n    def reset_stats(self):\n        self.total_gt = 0\n        self.total_fp = 0\n        self.total_fn = 0\n        self.total_matches = 0\n        self.iou_sum = 0.0\n        self.id_matches = 0\n        self.total_ids = 0\n\n    def load_gt(self, gt_path):\n        \"\"\"Loads COCO JSON and maps image_id -> (video_id, frame_index)\"\"\"\n        print(f\"üìÇ Loading GT from {gt_path}...\")\n        with open(gt_path, 'r') as f:\n            data = json.load(f)\n            \n        # Step A: Build Image Map (image_id -> video_id, frame_index)\n        image_map = {}\n        images = data.get('images', [])\n        \n        print(f\"   - Processing {len(images)} images map...\")\n        for img in images:\n            img_id = img['id']\n            # Try getting video_id directly, or parse from file_name\n            video_id = img.get('video_id')\n            file_name = img.get('file_name', '')\n            \n            # Fallback: Extract video_id from filename if missing (e.g., \"video_12/004.jpg\")\n            if video_id is None:\n                v_match = re.search(r'video_(\\d+)', file_name)\n                video_id = int(v_match.group(1)) if v_match else 0\n                \n            # Try getting frame_index directly, or parse from file_name\n            frame_index = img.get('frame_index')\n            if frame_index is None:\n                # Heuristic: assume filename ends in number (e.g. \"000134.jpg\")\n                f_match = re.search(r'(\\d+)\\.(jpg|png|jpeg)$', file_name)\n                frame_index = int(f_match.group(1)) if f_match else 0\n            \n            image_map[img_id] = (int(video_id), int(frame_index))\n\n        # Step B: Load Annotations using map\n        annotations = data.get('annotations', [])\n        print(f\"   - Mapping {len(annotations)} annotations...\")\n        \n        count = 0\n        for ann in annotations:\n            img_id = ann['image_id']\n            if img_id not in image_map:\n                continue\n            \n            vid_id, frame_idx = image_map[img_id]\n            \n            # Support both 'bbox' [x,y,w,h] and 'rect'\n            bbox = ann.get('bbox', ann.get('rect'))\n            if bbox:\n                self.gt_data[vid_id][frame_idx].append({\n                    'track_id': ann.get('track_id', ann.get('id')), # Use 'id' if 'track_id' missing\n                    'bbox': bbox\n                })\n                count += 1\n                \n        print(f\"‚úÖ Successfully loaded {count} GT objects for {len(self.gt_data)} videos.\")\n\n    def compute_iou(self, box1, box2):\n        x1, y1, w1, h1 = box1\n        x2, y2, w2, h2 = box2\n        xi1, yi1 = max(x1, x2), max(y1, y2)\n        xi2, yi2 = min(x1+w1, x2+w2), min(y1+h1, y2+h2)\n        inter = max(0, xi2-xi1) * max(0, yi2-yi1)\n        union = w1*h1 + w2*h2 - inter\n        return inter / (union + 1e-6)\n\n    def evaluate_frame(self, video_id, frame_idx, pred_tracks):\n        # Retrieve GT for specific video AND frame\n        gt_frame = self.gt_data[video_id][frame_idx]\n        \n        self.total_gt += len(gt_frame)\n        if not gt_frame:\n            self.total_fp += len(pred_tracks)\n            return\n        if not pred_tracks:\n            self.total_fn += len(gt_frame)\n            return\n            \n        gt_matched = [False] * len(gt_frame)\n        pred_matched = [False] * len(pred_tracks)\n        \n        for i, gt in enumerate(gt_frame):\n            best_iou, best_j = 0, -1\n            for j, pred in enumerate(pred_tracks):\n                if pred_matched[j]: continue\n                iou = self.compute_iou(gt['bbox'], pred['bbox'])\n                if iou > best_iou and iou > self.iou_threshold:\n                    best_iou, best_j = iou, j\n            \n            if best_j != -1:\n                gt_matched[i] = True\n                pred_matched[best_j] = True\n                self.total_matches += 1\n                self.iou_sum += best_iou\n                if gt['track_id'] == pred_tracks[best_j]['track_id']:\n                    self.id_matches += 1\n                self.total_ids += 1\n                \n        self.total_fn += gt_matched.count(False)\n        self.total_fp += pred_matched.count(False)\n\n    def get_metrics(self):\n        if self.total_gt == 0: return {'Status': 'No GT found for processed videos'}\n        mota = 1 - (self.total_fn + self.total_fp) / self.total_gt\n        idf1 = 2*self.id_matches / (self.total_gt + self.total_matches + self.total_fp + 1e-6) # Approx\n        return {\n            'MOTA': round(mota, 3), \n            'IDF1': round(idf1, 3),\n            'GT': self.total_gt, \n            'Matches': self.total_matches,\n            'FN': self.total_fn, \n            'FP': self.total_fp\n        }\n\n# ‚úÖ 3. INFERENCE LOOP\ndef run_novel_tracking_inference(model_path, videos_folder, output_folder, gt_path=None):\n    model = YOLO(model_path)\n    tracker = NovelByteTrack(track_thresh=0.4, track_buffer=30)\n    \n    evaluator = RealTimeEvaluator()\n    if gt_path: evaluator.load_gt(gt_path)\n\n    os.makedirs(output_folder, exist_ok=True)\n    video_files = list(Path(videos_folder).glob(\"*.mp4\"))\n    \n    all_preds = []\n    \n    for video_path in video_files:\n        # Extract video ID safely\n        try:\n            video_id = int(re.search(r'video_(\\d+)', video_path.name).group(1))\n        except:\n            video_id = 0 # Default if pattern doesn't match\n            \n        print(f\"\\nüé¨ Processing Video ID: {video_id} ({video_path.name})\")\n        \n        cap = cv2.VideoCapture(str(video_path))\n        w, h, fps = int(cap.get(3)), int(cap.get(4)), int(cap.get(5))\n        out = cv2.VideoWriter(f\"{output_folder}/tracked_{video_path.name}\", cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n        \n        frame_idx = 0\n        while cap.isOpened():\n            ret, frame = cap.read()\n            if not ret: break\n            \n            # Inference\n            results = model(frame, verbose=False, conf=0.25)[0]\n            detections = sv.Detections.from_ultralytics(results)\n            tracks = tracker.update_with_detections(detections)\n            \n            # Prepare Predictions\n            frame_preds = []\n            if len(tracks) > 0 and tracks.tracker_id is not None:\n                for i, tid in enumerate(tracks.tracker_id):\n                    if tid == -1: continue\n                    bbox = tracks.xyxy[i]\n                    frame_preds.append({\n                        'track_id': int(tid),\n                        'bbox': [float(bbox[0]), float(bbox[1]), float(bbox[2]-bbox[0]), float(bbox[3]-bbox[1])]\n                    })\n                    \n                    # Save for JSON output\n                    all_preds.append({\n                        'video_id': video_id,\n                        'frame_index': frame_idx,\n                        'track_id': int(tid),\n                        'bbox': frame_preds[-1]['bbox']\n                    })\n\n            # Evaluate (Pass video_id AND frame_idx)\n            if gt_path:\n                evaluator.evaluate_frame(video_id, frame_idx, frame_preds)\n            \n            # Visualize\n            annotated = frame.copy()\n            if len(tracks) > 0:\n                annotated = sv.BoxAnnotator().annotate(annotated, tracks)\n                labels = [f\"#{tid}\" for tid in tracks.tracker_id]\n                annotated = sv.LabelAnnotator().annotate(annotated, tracks, labels=labels)\n            \n            # Live Metrics\n            if gt_path:\n                metrics = evaluator.get_metrics()\n                text = f\"MOTA: {metrics.get('MOTA', 0)} | FN: {evaluator.total_fn} | FP: {evaluator.total_fp}\"\n                cv2.putText(annotated, text, (10, 30), 0, 0.6, (0, 255, 0), 2)\n            \n            out.write(annotated)\n            frame_idx += 1\n            \n        cap.release()\n        out.release()\n        print(f\"   --> Done. Frames: {frame_idx}\")\n\n    # Save Results\n    with open(f\"{output_folder}/predictions.json\", 'w') as f:\n        json.dump({'predictions': all_preds, 'metrics': evaluator.get_metrics() if gt_path else {}}, f)\n        \n    print(\"\\n‚úÖ Final Metrics:\", evaluator.get_metrics() if gt_path else \"Skipped\")\n    return all_preds\n\nif __name__ == \"__main__\":\n    MODEL_PATH = \"/kaggle/input/enhanced-yolov8-l-on-seadronessee/pytorch/v1/6/last.pt\"\n    # Update these paths to match your actual structure\n    VIDEOS_FOLDER = \"/kaggle/working/videos/kaggle/working/videos\"  \n    OUTPUT_FOLDER = \"/kaggle/working/novel_output\"\n    GT_PATH = \"/kaggle/input/seadronesseemot-dataset-video-conversion/unified_mot_dataset.json\"\n    \n    run_novel_tracking_inference(MODEL_PATH, VIDEOS_FOLDER, OUTPUT_FOLDER, GT_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T09:59:09.402542Z","iopub.execute_input":"2025-12-27T09:59:09.402888Z","iopub.status.idle":"2025-12-27T09:59:10.413474Z","shell.execute_reply.started":"2025-12-27T09:59:09.402862Z","shell.execute_reply":"2025-12-27T09:59:10.412723Z"}},"outputs":[{"name":"stdout","text":"üìÇ Loading GT from /kaggle/input/seadronesseemot-dataset-video-conversion/unified_mot_dataset.json...\n   - Processing 35843 images map...\n   - Mapping 207132 annotations...\n‚úÖ Successfully loaded 207132 GT objects for 20 videos.\n\nüöÄ Starting Inference Simulation...\n\nüé¨ Processing Video ID: 19 (video_019.mp4)\n   --> Done. Frames: 913\n\nüé¨ Processing Video ID: 21 (video_021.mp4)\n   --> Done. Frames: 7870\n\nüé¨ Processing Video ID: 13 (video_013.mp4)\n   --> Done. Frames: 291\n\nüé¨ Processing Video ID: 7 (video_007.mp4)\n   --> Done. Frames: 1001\n\nüé¨ Processing Video ID: 18 (video_018.mp4)\n   --> Done. Frames: 6028\n\nüé¨ Processing Video ID: 14 (video_014.mp4)\n   --> Done. Frames: 1726\n\nüé¨ Processing Video ID: 15 (video_015.mp4)\n   --> Done. Frames: 252\n\nüé¨ Processing Video ID: 9 (video_009.mp4)\n   --> Done. Frames: 3709\n\nüé¨ Processing Video ID: 5 (video_005.mp4)\n   --> Done. Frames: 715\n\nüé¨ Processing Video ID: 0 (video_000.mp4)\n   --> Done. Frames: 410\n\nüé¨ Processing Video ID: 8 (video_008.mp4)\n   --> Done. Frames: 1001\n\nüé¨ Processing Video ID: 12 (video_012.mp4)\n   --> Done. Frames: 715\n\n‚úÖ Final Metrics: {'MOTA': 0.584, 'IDF1': 0.642, 'GT': 207132, 'Matches': 175316, 'FN': 31816, 'FP': 19480, 'MT_Ratio': 0.45, 'ML_Ratio': 0.18, 'FPS': 38}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}